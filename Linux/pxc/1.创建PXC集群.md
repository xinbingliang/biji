# 创建PXC集群

centos7.0 pxc5.7

* MariaDB
* Percona

MobaXterm链接工具

## 安装PerconaServer数据库

* [安装说明文档](https://www.percona.com/doc/percona-server/5.7/installation/apt_repo.html)

* `apt-get install mysql-client`

* `ufw disable`

* 配置

  ````
  # /etc/mysql/percona-server.conf.d/mysqld.cnf
  character_set_server=utf8
  bind-address=0.0.0.0
  # 跳过DNS解析
  skip-name-resolve
  ````

* `service mysql restart`

* `chkconfig mysqld off` 防止宕机后重启的自动同步

* `systemctl disable mysql` 在ubuntu下

长时间宕机后应该拷贝数据库文件后启动

* `create user 'admin'@'%' identified by '123456';`
* `grant all privileges ON *.* To 'admin'@'%';`
* `flush privileges;`

## 安装PXC组建集群

* 节点配置应该相同
* 需要开放的端口
  * 3306 MySQL服务端口
  * 4444 请求全量同步(SST)端口，引发集群限速
  * 4567 数据库之间通信端口
  * 4568 请求增量同步(IST端口)
* [安装文档](https://www.percona.com/doc/percona-xtradb-cluster/5.7/install/apt.html#apt)

````
wget https://repo.percona.com/apt/percona-release_latest.$(lsb_release -sc)_all.deb
sudo dpkg -i percona-release_latest.$(lsb_release -sc)_all.deb
sudo apt-get update
````

* `service mysql stop` 停止mysql防止同步

````
node1 192.168.232.167
node2 192.168.232.168
node3 192.168.232.169
````

````
# /etc/mysql/percona-xtradb-cluster.conf.d/wsrep.cnf
server-id=3  #PXC集群中MySQL实例的唯一ID，不能重复，必须是数字
wsrep_provider=/usr/lib/galera3/libgalera_smm.so
wsrep_cluster_name=pxc-cluster  #PXC集群的名称
wsrep_cluster_address=gcomm://192.168.232.167,192.168.232.168,192.168.232.169
wsrep_node_name=pxc3  #当前节点的名称
wsrep_node_address=192.168.232.169  #当前节点的IP
wsrep_sst_method=xtrabackup-v2  #同步方法（mysqldump、rsync、xtrabackup）
wsrep_sst_auth= admin:123456  #同步使用的帐户
pxc_strict_mode=ENFORCING  #同步严厉模式
binlog_format=ROW  #基于ROW复制（安全可靠）
default_storage_engine=InnoDB  #默认引擎
innodb_autoinc_lock_mode=2  #主键自增长不锁表
````

* `/etc/init.d/mysql bootstrap-pxc` 首节点启动
* `service mysql start` 首节点启动数据库
* `service mysql start` 启动其他节点

## PXC集群的常用管理-数据库集群使用

### 同步信息

* `show status like 'wsrep_cluster%';` 查看集群节点信息
  * `show status like '%wsrep%';`
    * `wsrep_last_applied` 同步应用的次数
    * `wsrep_last_committed` 事务提交次数
    * `wsrep_replicated` 被其他节点复制的总数
    * `wsrep_replicated_bytes` 被其他节点复制的数据总数
    * `wsrep_received` 从其他节点处收到的总请求总数
    * `wsrep_received_bytes` 从其他节点处收到的数据总数

### 队列信息

* `wsrep_local_send_queue` 发送队列的长度
* `wsrep_local_send_queue_max` 发送队列的最大长度
* `wsrep_local_send_queue_min` 发送队列的最小长度
* `wsrep_local_send_queue_avg` 发送队列的平均长度，第一个参数和这个同时很大说明效率用问题
* `wsrep_local_recv_queue` 接收队列的长度
* `wsrep_local_recv_queue_max` 接收队列的最大长度
* `wsrep_local_recv_queue_min` 接收队列的最小长度
* `wsrep_local_recv_queue_avg` 接收队列的平均长度

### 流量控制

例如：节点添加会引发，应该拷贝数据文件

* `wsrep_flow_control_paused_ns` 流控暂停状态下花费的总时间
* `wsrep_flow_control_paused` 流量控制暂停时间的占比（0~1），越大流控占比越多
* `wsrep_flow_control_sent` 发送的流控暂停事件的数量
* `wsrep_flow_control_recv` 接受的流控暂停事件的数量
* `wsrep_flow_control_interval` 流量控制的下限和上限。上限 是队列中允许的最大请求数。如果队列达到上限，则会拒绝新的请求。当处理现有请求时，队列会减少，一旦达到下限，将再次运行新的请求
* `wsrep_flow_control_status` 流量控制的状态

解决方案：

* 提高网络环境
* 增加线程
  * `wsrep_slave_threads=16` 线程数量，CPU线程数的1~1.5倍

## PXC集群的常用管理-状态参数

### 状态参数

* `wsrep_local_state_comment` 节点状态
* `wsrep_cluster_staus` 集群状态（PRIMARY、NON_PRIMARY、Disconnected）
* `wsrep_connected` 节点是否链接到集群
* `wsrep_ready` 集群是否正常工作
* `wsrep_cluster_size` 节点数量
* `wsrep_desync_count` 延时节点数量
* `wsrep_incoming_addresses` 集群节点IP

### 事务相关

* `wsrep_cert_deps_distance` 事务并发数
* `wsrep_apply_oooe` 接收队列中事务占比
* `wsrep_apply_oool` 接收队列中事务乱序执行的频率
* `wsrep_apply_window` 接收队列中事务的平均数量
* `wsrep_commit_oooe` 发送队列中事务的占比
* `wsrep_commit_oool` 无任何意义

## PXC节点的上线与关闭

节点如何启动就如何关闭

* `/var/lib/mysql/grastate.dat` 当safe_to_bootstrap为1时作为主节点
* `/etc/init.d/mysql stop` 关闭主节点

启动时要先启动为1时

若全部为0，先修改一个为1后启动

## MySQL集群中间件比较

* 负载均衡类
  * 负载均衡提供了请求转发，降低了单节点的负载
  * Haproxy
  * MySQL-Proxy
* 数据切分类
  * 按照不同的路由算法分发SQL语句形成数据切分，防止单表2000万的问题，将一个集群当作一个分片，分片上使用中间件，注意冷热数据，冷数据不占用分片进入到归档数据库
  * MyCat 
  * Atlas
  * OneProxy
  * ProxySQL

## 配置MyCat负载均衡

```
192.168.2.118,192.168.2.147,192.168.2.109
```

* `sudo apt-get install -y openjdk-8-jre`
* `sudo apt-get install -y openjdk-8-jkd`

```
# /etc/profile
#set jdk environment 
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export JRE_HOME=$JAVA_HOME/jre 
export CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH 
export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH 
```

- `source /etc/profile `
- `[http://www.mycat.io](http://www.mycat.io/)` 下载地址

`````
mycat/
├── bin # 命令
├── catlet
├── conf # 配置文件
│   ├── rule.xml # 切分算法，将mod-log修改为分片数量
│   ├── server.xml # 虚拟MySQL，修改用户名，密码和逻辑库
│   ├── schema.xml # 数据库连接，读写分离，负载均衡，数据表映射
├── lib
├── logs # 日志
└── version.txt
`````

`````xml
<!-- server.xml -->
	<user name="admin" defaultAccount="true">  <!-- 配置虚拟用户 -->
		<property name="password">123456</property>
		<property name="schemas">user</property> <!-- 虚拟的数据库 -->
		
		<!-- 表级 DML 权限设置 -->
		<!-- 		
		<privileges check="false">
			<schema name="TESTDB" dml="0110" >
				<table name="tb01" dml="0000"></table>
				<table name="tb02" dml="1111"></table>
			</schema>
		</privileges>		
		 -->
	</user>


<!-- 只读用户，非必须 -->
<!-- 	<user name="user">
		<property name="password">user</property>
		<property name="schemas">TESTDB</property>
		<property name="readOnly">true</property>
	</user>
 -->

`````

````xml
<!-- schema.xml -->
<!-- 定义虚拟的逻辑库及虚拟的逻辑表 -->
<schema name="test" checkSQLschema="false" sqlMaxLimit="100"> <!-- 定义虚拟的逻辑库 -->
		<table name="t_user" dataNode="dn1,dn2,dn3" rule="mod-long" /> <!-- 定义虚拟的逻辑表,dataNode:分片，rule:切分算法 -->
</schema>


<!-- name:分片的名称，dataHost:对应下面dataHost的name, databas:被操作的真实数据库-->
<dataNode name="dn1" dataHost="mycat1" database="db1" /> <!-- 定义的分片 -->
<dataNode name="dn2" dataHost="mycat2" database="db1" /> <!-- 定义的分片 -->

<!-- 数据库的连接信息 -->
	<!-- name:自定义一连接信息名称 maxCon minCon 数据库连接池信息 balance：负载均衡类型，0表示不开启读写分离，为1读写分离，为2每个节点同时负责读写，writeType：0表示所有写请求分发给第一个写节点，1表示写操作由所有写节点操作，switchType：切换节点的的依据，1使用mycat自己的心跳检测，用数据库集群的信息判断 -->
	<dataHost name="mycat1" maxCon="1000" minCon="10" balance="2:wq"
			  writeType="1" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
		<heartbeat>select user()</heartbeat> <!-- 心跳检测的sql语句 -->
		<writeHost host="pxc1" url="192.168.232.167:3306" user="admin"
				   password="123456"> <!-- 配置写节点 -->
			<readHost host="pxc2" url="192.168.232.168:3306" user="admin" password="123456" /> <!-- 配置读节点 -->
          	<readHost host="pxc3" url="192.168.232.169:3306" user="admin" password="123456" /> <!-- 配置读节点 -->
		</writeHost>
      <!-- 一个冗余设计 -->
		<writeHost host="pxc2" url="192.168.232.168:3306" user="admin"
				   password="123456"> <!-- 配置写节点 -->
			<readHost host="pxc1" url="192.168.232.167:3306" user="admin" password="123456" /> <!-- 配置读节点 -->
          	<readHost host="pxc3" url="192.168.232.169:3306" user="admin" password="123456" /> <!-- 配置读节点 -->
		</writeHost>
	</dataHost>

<!-- 另外一个分片 -->
	<dataHost name="mycat2" maxCon="1000" minCon="10" balance="2"
			  writeType="1" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
		<heartbeat>select user()</heartbeat> <!-- 心跳检测的sql语句 -->
		<writeHost host="pxc1" url="192.168.232.167:3306" user="admin"
				   password="123456"> <!-- 配置写节点 -->
			<readHost host="pxc2" url="192.168.232.168:3306" user="admin" password="123456" /> <!-- 配置读节点 -->
          	<readHost host="pxc3" url="192.168.232.169:3306" user="admin" password="123456" /> <!-- 配置读节点 -->
		</writeHost>
      <!-- 一个冗余设计 -->
		<writeHost host="pxc2" url="192.168.232.168:3306" user="admin"
				   password="123456"> <!-- 配置写节点 -->
			<readHost host="pxc1" url="192.168.232.167:3306" user="admin" password="123456" /> <!-- 配置读节点 -->
          	<readHost host="pxc3" url="192.168.232.169:3306" user="admin" password="123456" /> <!-- 配置读节点 -->
		</writeHost>
	</dataHost>
````

`````xml
# rule.xml
<function name="mod-long" class="io.mycat.route.function.PartitionByMod">
  <!-- how many data nodes -->
  <property name="count">2</property> <!-- 对应你的分片数量 -->
</function>
`````

* `8066` 数据服务接口，也即数据读写连接端口
* `9066` 管理端口
* `cd bin && chomd -R 777 ./*.sh`
* `./startup_nowrap.sh`
* `ps -aux`
* 杀死进程即关闭

## 数据切分

| 切分算法   | 适用场合           | 备注     |
| ------ | -------------- | ------ |
| 主键求模切分 | 数据增长速度慢，难于增加分片 | 有明确主键值 |
| 枚举值切分  | 归类存储数据，适合大多数业务 |        |
| 主键范围切分 | 数据快速增长，容易增加分片  | 有明确主键值 |
| 日期切分   | 数据快速增长，容易增加分片  |        |

### 主键求模切分

* 球磨切分适合数据量大，但数据增长不快的场景，地图产品、行政数据、企业数据
* 扩展分片难度大，迁移数据量大

### 枚举值切分

* 按照某字段的值（数字）来切分数据，用来归类数据

````xml
	<tableRule name="sharding-by-intfile"><!-- 名称可以自由修改 -->
		<rule>
			<columns>sharding_id</columns> <!-- 数据表中特定的字段名称 -->
			<algorithm>hash-int</algorithm> <!-- 对应的底层算法 -->
		</rule>
	</tableRule>
````

````
# /mycat/conf/customer-hash-init.txt
101=0
102=0
103=0
104=1
105=1
101=1
````

````xml
<tableRule name="sharding-by-intfile">
  <rule>	
  	<columns>sharding_id</columns>
  	<algorithm>hash-int</algorithm>
  </rule>
</tableRule>

<tableRule name="sharding-customer">
  <rule>
  	<columns>sharding_id</columns> <!-- 指定的字段 -->
  	<algorithm>customer-hash-int</algorithm>
  </rule>
</tableRule>

.....
<function name="hash-int"
		class="io.mycat.route.function.PartitionByFileMap">
		<property name="mapFile">partition-hash-int.txt</property>
</function>
<function name="customer-hash-int"
		class="io.mycat.route.function.PartitionByFileMap">
		<property name="mapFile">customer-hash-init.txt</property>
</function>
````

* `mysql$:reload @@config_all;` 重新热加载

### 主键范围切分

### 日期切分

## 父子表

在MyCat中不允许跨分片做表连接查询

父子表：对父表进行切分对子表不允许切分

````xml
<!-- schema.xml -->
<schema name="test" checkSQLschema='false' sqlMaxLimit="100">
    <table name="t_customer" dataNode="dn1,dn2" rule="sharding-customer">
        <childTable name="t_orders" primaryKey="ID" joinKey="customer_id"   
                    parentKey="id"/> <!-- primaryKey:非必须，joinKey子表字段，parentKey父表字段，子表可以包含字表，也可以有多个字表 -->
    </table>
</schema>
````

## 组建双机热备的MyCat集群-构建高可用的MyCat集群

![](./images/6.png)

使用虚拟IP，实现抢占IP

Haproxy每秒转发八万次请求，Mysql要大量集群，MyCat要少量集群，Haproxy不需要集群

### haproxy 

可以选择部署到新的机器上也可以和myca同机器

* 关闭防火墙

  * 3306 TCP/IP转发端口
  * 4001 监控界面

* `apt-get install haproxy`

* `vim /etc/haproxy/haproxy.cfg`

  `````
  global
      log         127.0.0.1 local2
      chroot      /var/lib/haproxy
      pidfile     /var/run/haproxy.pid
      maxconn     4000
      user        haproxy
      group       haproxy
      daemon
      # turn on stats unix socket
      stats socket /var/lib/haproxy/stats

  defaults
      mode                    http
      log                     global
      option                  httplog
      option                  dontlognull
      option http-server-close
      option forwardfor       except 127.0.0.0/8
      option                  redispatch
      retries                 3
      timeout http-request    10s
      timeout queue           1m
      timeout connect         10s
      timeout client          1m
      timeout server          1m
      timeout http-keep-alive 10s
      timeout check           10s
      maxconn                 3000

  # 监控服务设置
  listen   admin_stats  
      bind    0.0.0.0:4001 # 监控界面访问的IP和端口
      mode  http # 访问协议
      stats uri       /dbs # URI相对地址
      stats realm  Global\ statistics # 统计报告格式
      stats auth    admin:abc123456 # 登录帐号信息
  listen   proxy-mysql
      bind    0.0.0.0:3306  
      mode  tcp 
      # 轮训算法：roundrobin
      # 权重算法：static-rr
      # 最少连接算法：leastconn
      # 请求源IP算法：source
      balance  roundrobin # 使用轮训算法进行请求转发
      option  tcplog       # 日志格式
      server   mycat_1  192.168.99.131:3306  check  port  8066  weight  1  maxconn  2000  # mycat的负载均衡设置，8066为心跳检测端口，权重在权重算法中有效
      server   mycat_2  192.168.99.132:3306  check  port  8066  weight  1  maxconn  2000  
      option  tcpka        #使用keepalive检测死链
  `````

* `service haproxy start`

## 组建双机热备的MyCat集群-利用keepalived抢占虚拟IP

* 关闭防火墙

* `apt-get install keepalived`

* `vim /etc/keepalived/keepalived.conf`

  ````
  vrrp_instance  VI_1 { # 这一组配置的名称
      state  MASTER # 表示keepalived要争抢IP，当启动时，就会和其他节点一起抢占虚拟IP
      interface  ens33 # 对应网卡名称
      virtual_router_id  51 # 虚拟路由的标识，设备的ID号，0-255之间，两个节点不同
      priority  100 # 优先级，一种权重
      advert_int  1 # 心跳检测间隔，秒
      authentication { # 心跳检测的账户名
          auth_type  PASS
          auth_pass  123456
      }
      virtual_ipaddress { # 定义虚拟IP的地址
          192.168.99.133
      }
  }
  ````

* `service keepalived start` 

* 通过上述设置的虚拟IP能访问，使用mycat用户名密码

* `service keepalived stop` 模拟宕机

## Sysbench基准测试-安装Sysbench

* QPS：每秒能处理完成的请求次数
* TPS ：每秒钟执行完成的事务次数
* 响应时间：一次请求所需要的平均处理时间
* 并发量：系统能同时处理的请求数

### Sysbench测试

* `sysbench --test=threads --num-threads=64 --thread-yields=100 --thread-locks=2 run` 线程测试
* `sysbench --test=cpu --cpu-max-prime=20000 run` CPU测试
* `sysbench --test=oltp --mysql-table-engine=myisam --oltp-table-size=1000000 ` 数据库测试
* `sysbench --test=memory --memory-block-size=8k --memory-total-size=4G run` 内存测试
* `sysbench --test=fileio --num-threads=16 --file-total-size=3G --file-test-mode=rndrw run` 磁盘测试

#### 安装

* `apt-get install sysbench`

## Sysbench基准测试-使用Sysbench

### 连接参数

|       参数名称       | 功能意义 |
| :--------------: | :--: |
|   --mysql-host   | IP地址 |
|   --mysql-port   | 端口号  |
|   --mysql-user   | 用户名  |
| --mysql-password |  密码  |

### 执行参数

|        参数名称         |                   功能意义                   |
| :-----------------: | :--------------------------------------: |
|  --oltp-test-mode   | 执行模式（simple【测试查询，不测试写入】、nontrx【测试无事务增删改查】、complex【有事务增删改查】） |
| --oltp-tables-count |                  测试表的数量                  |
|  --oltp-table-size  |                 测试表的记录数                  |
|      --threads      |                  并发连接数                   |
|       --time        |                测试执行时间（秒）                 |
|  --report-interval  |              生成报告单的间隔时间（秒）               |

|  命令名称   |  功能意义  |
| :-----: | :----: |
| prepare | 准备测试数据 |
|   run   | 执行测试数据 |
| cleanup | 清除测试数据 |

* `vim /etc/haproxy/haproxy.cfg` 对数据库节点做负载均衡

````
    option  tcplog       # 日志格式
    server   mysql_1  192.168.99.131:3306  check  port  8066  weight  1  maxconn  2000  # mycat的负载均衡设置，8066为心跳检测端口，权重在权重算法中有效
    server   mysql_1  192.168.99.132:3306  check  port  8066  weight  1  maxconn  2000  
    option  tcpka        #使用keepalive检测死链
````

* `sysbench  /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-host=192.168.99.131 --mysql-port=3306 --mysql-user=admin --mysql-password=Abc_123456 --oltp-tables-count=10 --oltp-table-size=100000 prepare` 准备测试库
* `sysbench  /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-host=192.168.99.131 --mysql-port=3306 --mysql-user=admin --mysql-password=Abc_123456 --oltp-test-mode=complex --threads=10 --time=300 --report-interval=10 run >> /home/mysysbench.log` 执行测试
* `sysbench  /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-host=192.168.99.131 --mysql-port=3306 --mysql-user=admin --mysql-password=Abc_123456 --oltp-test-mode=complex --threads=10 --time=300 --report-interval=10 run >> /home/mysysbench.log` 清理数据

实际测试24小时以上

## tpcc-mysql压力测试







