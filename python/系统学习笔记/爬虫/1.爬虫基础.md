# 爬虫

**网页三大特征：**

* 网页都有自己唯一的URL（统一资源定位符）来进行定位
* 网页都使用HTML （超文本标记语言）来描述页面信息。
* 网页都使用HTTP/HTTPS（超文本传输协议）协议来传输HTML数据。

**爬虫的设计思路：**

* 首先确定需要爬取的网页URL地址。
* 通过HTTP/HTTP协议来获取对应的HTML页面。
* 提取HTML页面里有用的数据：
  * a. 如果是需要的数据，就保存起来。
  * b. 如果是页面里的其他URL，那就继续执行第二步。

**抓取库**

* `urllib`、`urllib2`、`requests`

**解析库**

* `re`、`xpath`、`BeautifulSoup4(bs4)`、`jsonpath` 、`pyquery`

**动态页面**

* `Selenium+Phantomjs` 模拟真实浏览器
* `Tesseract` 机器学习库，进行图像识别

**框架**

* `Scrapy（twisted）/Pyspider` 

**分布式策略**

* `scrapy-redis`

**爬虫分类**

1. 通用爬虫（搜索引擎）
2. 聚焦爬虫（程序员针对部分内容做的爬虫）

## urllib2 

网络抓取库，在3中你被改为`urllib.request`

默认库安装位置`/usr/lib/python2.7/urllib2.py`

`/usr/local/lib/python2.7/site-paceages/` pip安装后存的位置

* `urllib2.urlopen(url, data=, timeout)`
  * url 请求链接
  * data post请求时的参数
  * timeout 超时时间


````python
# -*- coding:utf8 -*-
import urllib2

# 向指定的url地址发起请求，并返回服务器响应的类文件对象
response = urllib2.urlopen("http://www.baidu.com")

# 服务器类文件对象支持Python文件对象的操作方法
# read()读取文件内容，返回字符串
html = response.read()

print html
````

**以上不支持自定义构造HTTP请求，默认User-Agent会是：Python-urllib**

* `Accept-Encoding` 不要写，避免数据压缩
* `User-Agent` 必须写，避免爬虫被识别
* `http://www.baidu.com/` 末尾斜杠加上

````python
# -*- coding:utf8 -*-
import urllib2

user_headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
}

#通过urllib2.Request()方法构造一个请求对象
request = urllib2.Request("http://www.baidu.com", headers=user_headers)

# 向指定的url地址发起请求，并返回服务器响应的类文件对象
response = urllib2.urlopen(request)

# 服务器类文件对象支持Python文件对象的操作方法
# read()读取文件内容，返回字符串
html = response.read()

print html
````

* `response.getcode()` 返回`http`请求的返回状态码
* `response.geturl()` 返回数据的是哪个URL，防止重定向
* `response.info()` 返回服务器响应的`http`报头

### 反爬虫

```python
# -*- coding:utf8 -*-
import urllib2
import random

url = "http://www.baidu.com"

ua_list = [
    "Mozilla/5.0(Macintosh;U;IntelMacOSX10_6_8;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50",
    "Mozilla/5.0(Windows;U;WindowsNT6.1;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50",
    "Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0",
    "Mozilla/5.0(Macintosh;IntelMacOSX10.6;rv:2.0.1)Gecko/20100101Firefox/4.0.1",
    "Opera/9.80(WindowsNT6.1;U;en)Presto/2.8.131Version/11.11",
    "Mozilla/5.0(Macintosh;IntelMacOSX10_7_0)AppleWebKit/535.11(KHTML,likeGecko)Chrome/17.0.963.56Safari/535.11",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Maxthon2.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;TencentTraveler4.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Trident/4.0;SE2.XMetaSr1.0;SE2.XMetaSr1.0;.NETCLR2.0.50727;SE2.XMetaSr1.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;360SE)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;AvantBrowser)"
]


# 在可能的User-Agent列表中随机选择一个User-Agent
user_agent = random.choice(ua_list)

# 构造一个请求
request = urllib2.Request(url)

# 添加/修改一个头信息
request.add_header('User-Agent', user_agent)

# 获取一个已有的报头值
print request.get_header("User-agent")

```

## urlencode

* `urllib.urlencode()` 接受的参数是个字典

  ````python
  import urllib
  world={"name": "辛丙亮"}
  urllib.urlencode(world)
  m = urllib.urlencode(world)
  print urllib.unquote(m)
  name=辛丙亮
  ````

* `urllib.unquote() ` 转换回

````python
# -*- coding:utf8 -*-
import urllib2
import urllib

url = "http://www.baidu.com/s?"

keyword = raw_input("请输入用户的查询：")

wd = {"wd": keyword}

wd = urllib.urlencode(wd)

fullUrl = url + wd

request = urllib2.Request(fullUrl)
response = urllib2.urlopen(request)
print response.read()
````

## GET和POST

````python
# -*- coding:utf8 -*-
import urllib2
import urllib

url = "http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule"
key_world = raw_input("请输入要翻译的文本:")

form_data={
"i": key_world,
"type": "AUTO",
"doctype":"json",
"xmlVersion": "1.8",
"keyfrom":"fanyi.web",
"action":"FY_BY_CLICKBUTTION",
"typoResult":"true"
}

headers = {
"Accept":"application/json, text/javascript, */*; q=0.01",
"X-Requested-With":"XMLHttpRequest",
"User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36",
"Content-Type":"application/x-www-form-urlencoded; charset=UTF-8"
}

form_data_encode = urllib.urlencode(form_data)
request = urllib2.Request(url=url, data=form_data_encode, headers=headers)
response = urllib2.urlopen(request)
print response.read()
````

## Ajax

同POST

## cookie

```python
# -*- coding:utf8 -*-
import urllib
import urllib2


url = 'http://www.renren.com/477182930/profile'

headers = {
"Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"Host":"www.renren.com",
"Accept-Language":"zh-CN,zh;q=0.9",
"Connection":"keep-alive",
"Cookie":"anonymid=jbg3losyqqqiek; depovince=GW; jebecookies=8bcf7ac9-26d3-4017-ad5b-77b0fba085bd|||||; _r01_=1; ick_login=17783882-fd14-414b-8a08-1663950fe117; _de=BCE15886291E57C5B0771936531D403D; p=9dc3737732b56539578bcda6943cb3273; first_login_flag=1; ln_uact=15102724518; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=8cd63aa75136d5622eb31cdc5652baed3; societyguester=8cd63aa75136d5622eb31cdc5652baed3; id=812033673; xnsid=8844bbca; loginfrom=syshome; ch_id=10016; _ga=GA1.2.1048710234.1513837285; _gid=GA1.2.389901114.1513837285; wp_fold=0",
"User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
}


request = urllib2.Request(url=url, headers=headers)
response = urllib2.urlopen(request)

print response.read()
```

## HTTPS

```python
# 忽略证书
# -*- coding:utf8 -*-
import urllib2
import urllib
import ssl

# 2. 表示忽略未经核实的ssl证书认证
context = ssl._create_unverified_context();

url = "https://www.12306.cn/mormhweb/"

request = urllib2.Request(url)

# 在urlopen()中明确添加context
response = urllib2.urlopen(request, context=context)

print response.read()

```

## Handler处理器和自定义opener

urlopen()不支持代理、cookie等其他HTTP/HTTPS等高级功能

1. 使用相关的`gandler`处理器，来创建特定的功能处理器对象
2. 然后通过`urllib2.build_opener()` 方法使用这些处理器对象，创建opener对象
3. 使用自定义的opener对象，调用open()方法发送请求

```python
# -*- coding:utf8 -*-
import urllib2

# 构建一个HTTPHandler处理器对象
# http_handler = urllib2.HTTPHandler()
http_handler = urllib2.HTTPHandler(debuglevel=1) # 自动打开调试模式，打印收发报信息

# 调用build_opener()方法构建一个自定义的opener对象，参数是构建的处理器对象
opener = urllib2.build_opener(http_handler)

request = urllib2.Request("http://www.baidu.com/")

response = opener.open(request)

print response.read()
```

## 代理

````python
# -*- coding:utf8 -*-
import urllib2

proxySwitch = True #打开代理

# 构建一个handler处理器，参数是一个字典类型
#username:passwd@110.110.110.100：port
httpProxyHandler = urllib2.ProxyHandler({"http": "114.215.104.49:16816"})

# 没有代理的处理器对象
nullProxyHandler = urllib2.ProxyHandler({})

if proxySwitch:
    opener = urllib2.build_opener(httpProxyHandler)
else:
    opener = urllib2.build_opener(nullProxyHandler)

# 构建一个全局的opener,之后说有的请求都可以用urlopen打开
urllib2.install_opener(opener)

request = urllib2.Request("http://www.baidu.com/")
response = urllib2.urlopen(request)

print response.read()
````

## 代理和WEB客户端授权验证

服务器配置了登录名密码时

* `urllib2.HTTPPasswordMgrWithDefaultRealm()` 创建一个密码管理对象，用来保存Http请求相关授权信息
* `ProxyBasicAuthHandler()` 授权代理处理器
* `HTTPBasicAuthHandler()` 验证web客户端的授权处理器

```python
# -*- coding:utf8 -*-
import urllib2

test = "test"
passwd = '123456'
webserver = "192.168.21.56"

# 构建一个密码管理对象，用来保存和请求相关的授权账户信息
passwdMgr = urllib2.HTTPPasswordMgrWithDefaultRealm()

# 添加授权账户信息，第一个参数real如果没有指定就写None，站点IP，账户，密码
passwdMgr.add_password(None, webserver, test, passwd)

httpauth_handler = urllib2.HTTPBasicAuthHandler(passwdMgr)

opener = urllib2.build_opener(httpauth_handler) # 可以同时加上代理

request = urllib2.Request("http://"+webserver)

response = opener.open(request)

print response.read()
```

##模拟登录

### cookielib

该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。

> - CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。
> - FileCookieJar (filename,delayload=None,policy=None)：从CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。
> - MozillaCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与`Mozilla浏览器 cookies.txt兼容`的FileCookieJar实例。
> - LWPCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与`libwww-perl标准的 Set-Cookie3 文件格式`兼容的FileCookieJar实例。

```python
# -*- coding:utf8 -*-
import urllib2
import cookielib
import urllib

# 通过CookieJar()类来构建一个cookieJar()对象，用来执行cookie值
cookie = cookielib.CookieJar()

# 通过HTTPCookieProcess()处理器类构建一个处理器对象，来处理cookie
# 参数是构建的Cookiejar()对象
cookie_handler = urllib2.HTTPCookieProcessor(cookie)
# 构建一个自定义的opener
opener = urllib2.build_opener(cookie_handler)

# 自定义opener的参数，可以赋值Http报头参数
opener.addheaders = [
    ("User-Agent", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36")
]

url = 'http://www.renren.com/PLogin.do'

data = {
    "email": "709464835@qq.com",
    "password": "yjfc4883212"
}

# 编码数据
data = urllib.urlencode(data)

request = urllib2.Request(url=url, data=data)

response = opener.open(request)

# print response.read()

#第二次请求会使用上一次请求保存的cookie
response_frind = opener.open('http://www.renren.com/360072054/profile')

print response_frind.read()
```

## re

python中的re模块有两种方式

* ````
  pattern = re.compile("\d") # 将正则表达式编译成一个Pattern规则对象

  pattern.match() 从开始位置查找，第一个符合规则的
  pattern.search() 从任意位置往后查找，返回第一个
  pattern.findall() 全部匹配项，返回列表
  pattern.finditer() 所有全部匹配，返回一个迭代器
  pattern.split() 分割字符串，返回列表
  pattern.sub() 替换
  ````


```python
# -*- coding:utf8 -*-
import re

pattern = re.compile(r'\d+')
print pattern.match(r'123456', 2, 5).group()
```

## 段子爬虫

```python
# -*- coding:utf8 -*-
import urllib2
import re


class Spider:
    def __init__(self):
        self.switch = True # 爬虫开关
        self.page = 1

    def load_page(self):
        """下载页面"""
        url = 'http://www.neihan8.com/article/list_5_'+str(self.page)+'.html'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
        }

        request = urllib2.Request(url=url, headers=headers)
        response = urllib2.urlopen(request)
        html = response.read()

        # 创建正则表达式规则对象，匹配每页里的段子内容,re.S表示匹配全部字符串
        pattern = re.compile('<div\sclass="f18\smb20">(.*?)</div>', re.S)

        content_list = pattern.findall(html)

        self.deal_page(content_list)

    def deal_page(self, content_list):
        """处理每个页面的段子"""
        for item in content_list:
            item = item.decode('gbk').encode('utf-8')
            item = item.replace("<p>", "").replace("</p>","").replace("<br />", "").replace("&ldquo;", "").replace("&rdquo;","").replace("\r\n", "").replace("&hellip;", "")
            self.write_page(item)

    def write_page(self, item):
        """内容写入文件"""
        with open('duanzi.txt', 'a+') as f:
            f.write(item)

    def start_work(self):
        """控制爬虫运行"""
        while self.switch:
            command = raw_input("按回车爬取下一页，退出按quit：")

            if 'quit' == command:
                self.switch = False
            else:
                self.load_page()
                self.page += 1



if __name__ == "__main__":
    obj = Spider()
    obj.start_work()
```
















