# 爬虫

**网页三大特征：**

* 网页都有自己唯一的URL（统一资源定位符）来进行定位
* 网页都使用HTML （超文本标记语言）来描述页面信息。
* 网页都使用HTTP/HTTPS（超文本传输协议）协议来传输HTML数据。

**爬虫的设计思路：**

* 首先确定需要爬取的网页URL地址。
* 通过HTTP/HTTP协议来获取对应的HTML页面。
* 提取HTML页面里有用的数据：
  * a. 如果是需要的数据，就保存起来。
  * b. 如果是页面里的其他URL，那就继续执行第二步。

**抓取库**

* `urllib`、`urllib2`、`requests`

**解析库**

* `re`、`xpath`、`BeautifulSoup4(bs4)`、`jsonpath` 、`pyquery`

**动态页面**

* `Selenium+Phantomjs` 模拟真实浏览器
* `Tesseract` 机器学习库，进行图像识别

**框架**

* `Scrapy（twisted）/Pyspider` 

**分布式策略**

* `scrapy-redis`

**爬虫分类**

1. 通用爬虫（搜索引擎）
2. 聚焦爬虫（程序员针对部分内容做的爬虫）

## urllib2 

网络抓取库，在3中你被改为`urllib.request`

默认库安装位置`/usr/lib/python2.7/urllib2.py`

`/usr/local/lib/python2.7/site-paceages/` pip安装后存的位置

* `urllib2.urlopen(url, data=, timeout)`
  * url 请求链接
  * data post请求时的参数
  * timeout 超时时间


````python
# -*- coding:utf8 -*-
import urllib2

# 向指定的url地址发起请求，并返回服务器响应的类文件对象
response = urllib2.urlopen("http://www.baidu.com")

# 服务器类文件对象支持Python文件对象的操作方法
# read()读取文件内容，返回字符串
html = response.read()

print html
````

**以上不支持自定义构造HTTP请求，默认User-Agent会是：Python-urllib**

* `Accept-Encoding` 不要写，避免数据压缩
* `User-Agent` 必须写，避免爬虫被识别
* `http://www.baidu.com/` 末尾斜杠加上

````python
# -*- coding:utf8 -*-
import urllib2

user_headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
}

#通过urllib2.Request()方法构造一个请求对象
request = urllib2.Request("http://www.baidu.com", headers=user_headers)

# 向指定的url地址发起请求，并返回服务器响应的类文件对象
response = urllib2.urlopen(request)

# 服务器类文件对象支持Python文件对象的操作方法
# read()读取文件内容，返回字符串
html = response.read()

print html
````

* `response.getcode()` 返回`http`请求的返回状态码
* `response.geturl()` 返回数据的是哪个URL，防止重定向
* `response.info()` 返回服务器响应的`http`报头

### 反爬虫

```python
# -*- coding:utf8 -*-
import urllib2
import random

url = "http://www.baidu.com"

ua_list = [
    "Mozilla/5.0(Macintosh;U;IntelMacOSX10_6_8;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50",
    "Mozilla/5.0(Windows;U;WindowsNT6.1;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50",
    "Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0",
    "Mozilla/5.0(Macintosh;IntelMacOSX10.6;rv:2.0.1)Gecko/20100101Firefox/4.0.1",
    "Opera/9.80(WindowsNT6.1;U;en)Presto/2.8.131Version/11.11",
    "Mozilla/5.0(Macintosh;IntelMacOSX10_7_0)AppleWebKit/535.11(KHTML,likeGecko)Chrome/17.0.963.56Safari/535.11",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Maxthon2.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;TencentTraveler4.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Trident/4.0;SE2.XMetaSr1.0;SE2.XMetaSr1.0;.NETCLR2.0.50727;SE2.XMetaSr1.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;360SE)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;AvantBrowser)"
]


# 在可能的User-Agent列表中随机选择一个User-Agent
user_agent = random.choice(ua_list)

# 构造一个请求
request = urllib2.Request(url)

# 添加/修改一个头信息
request.add_header('User-Agent', user_agent)

# 获取一个已有的报头值
print request.get_header("User-agent")

```

## urlencode

* `urllib.urlencode()` 接受的参数是个字典

  ````python
  import urllib
  world={"name": "辛丙亮"}
  urllib.urlencode(world)
  m = urllib.urlencode(world)
  print urllib.unquote(m)
  name=辛丙亮
  ````

* `urllib.unquote() ` 转换回

````python
# -*- coding:utf8 -*-
import urllib2
import urllib

url = "http://www.baidu.com/s?"

keyword = raw_input("请输入用户的查询：")

wd = {"wd": keyword}

wd = urllib.urlencode(wd)

fullUrl = url + wd

request = urllib2.Request(fullUrl)
response = urllib2.urlopen(request)
print response.read()
````

## GET和POST

````python
# -*- coding:utf8 -*-
import urllib2
import urllib

url = "http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule"
key_world = raw_input("请输入要翻译的文本:")

form_data={
"i": key_world,
"type": "AUTO",
"doctype":"json",
"xmlVersion": "1.8",
"keyfrom":"fanyi.web",
"action":"FY_BY_CLICKBUTTION",
"typoResult":"true"
}

headers = {
"Accept":"application/json, text/javascript, */*; q=0.01",
"X-Requested-With":"XMLHttpRequest",
"User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36",
"Content-Type":"application/x-www-form-urlencoded; charset=UTF-8"
}

form_data_encode = urllib.urlencode(form_data)
request = urllib2.Request(url=url, data=form_data_encode, headers=headers)
response = urllib2.urlopen(request)
print response.read()
````

## Ajax

同POST

## cookie

```python
# -*- coding:utf8 -*-
import urllib
import urllib2


url = 'http://www.renren.com/477182930/profile'

headers = {
"Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"Host":"www.renren.com",
"Accept-Language":"zh-CN,zh;q=0.9",
"Connection":"keep-alive",
"Cookie":"anonymid=jbg3losyqqqiek; depovince=GW; jebecookies=8bcf7ac9-26d3-4017-ad5b-77b0fba085bd|||||; _r01_=1; ick_login=17783882-fd14-414b-8a08-1663950fe117; _de=BCE15886291E57C5B0771936531D403D; p=9dc3737732b56539578bcda6943cb3273; first_login_flag=1; ln_uact=15102724518; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=8cd63aa75136d5622eb31cdc5652baed3; societyguester=8cd63aa75136d5622eb31cdc5652baed3; id=812033673; xnsid=8844bbca; loginfrom=syshome; ch_id=10016; _ga=GA1.2.1048710234.1513837285; _gid=GA1.2.389901114.1513837285; wp_fold=0",
"User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
}


request = urllib2.Request(url=url, headers=headers)
response = urllib2.urlopen(request)

print response.read()
```

## HTTPS

```python
# 忽略证书
# -*- coding:utf8 -*-
import urllib2
import urllib
import ssl

# 2. 表示忽略未经核实的ssl证书认证
context = ssl._create_unverified_context();

url = "https://www.12306.cn/mormhweb/"

request = urllib2.Request(url)

# 在urlopen()中明确添加context
response = urllib2.urlopen(request, context=context)

print response.read()

```

## Handler处理器和自定义opener

urlopen()不支持代理、cookie等其他HTTP/HTTPS等高级功能

1. 使用相关的`gandler`处理器，来创建特定的功能处理器对象
2. 然后通过`urllib2.build_opener()` 方法使用这些处理器对象，创建opener对象
3. 使用自定义的opener对象，调用open()方法发送请求

```python
# -*- coding:utf8 -*-
import urllib2

# 构建一个HTTPHandler处理器对象
# http_handler = urllib2.HTTPHandler()
http_handler = urllib2.HTTPHandler(debuglevel=1) # 自动打开调试模式，打印收发报信息

# 调用build_opener()方法构建一个自定义的opener对象，参数是构建的处理器对象
opener = urllib2.build_opener(http_handler)

request = urllib2.Request("http://www.baidu.com/")

response = opener.open(request)

print response.read()
```

## 代理

````python
# -*- coding:utf8 -*-
import urllib2

proxySwitch = True #打开代理

# 构建一个handler处理器，参数是一个字典类型
#username:passwd@110.110.110.100：port
httpProxyHandler = urllib2.ProxyHandler({"http": "114.215.104.49:16816"})

# 没有代理的处理器对象
nullProxyHandler = urllib2.ProxyHandler({})

if proxySwitch:
    opener = urllib2.build_opener(httpProxyHandler)
else:
    opener = urllib2.build_opener(nullProxyHandler)

# 构建一个全局的opener,之后说有的请求都可以用urlopen打开
urllib2.install_opener(opener)

request = urllib2.Request("http://www.baidu.com/")
response = urllib2.urlopen(request)

print response.read()
````


















