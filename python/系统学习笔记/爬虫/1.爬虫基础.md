# 爬虫

**网页三大特征：**

* 网页都有自己唯一的URL（统一资源定位符）来进行定位
* 网页都使用HTML （超文本标记语言）来描述页面信息。
* 网页都使用HTTP/HTTPS（超文本传输协议）协议来传输HTML数据。

**爬虫的设计思路：**

* 首先确定需要爬取的网页URL地址。
* 通过HTTP/HTTP协议来获取对应的HTML页面。
* 提取HTML页面里有用的数据：
  * a. 如果是需要的数据，就保存起来。
  * b. 如果是页面里的其他URL，那就继续执行第二步。

**抓取库**

* `urllib`、`urllib2`、`requests`

**解析库**

* `re`、`xpath`、`BeautifulSoup4(bs4)`、`jsonpath` 、`pyquery`

**动态页面**

* `Selenium+Phantomjs` 模拟真实浏览器
* `Tesseract` 机器学习库，进行图像识别

**框架**

* `Scrapy（twisted）/Pyspider` 

**分布式策略**

* `scrapy-redis`

**爬虫分类**

1. 通用爬虫（搜索引擎）
2. 聚焦爬虫（程序员针对部分内容做的爬虫）

## urllib2 

网络抓取库，在3中你被改为`urllib.request`

默认库安装位置`/usr/lib/python2.7/urllib2.py`

`/usr/local/lib/python2.7/site-paceages/` pip安装后存的位置

* `urllib2.urlopen(url, data=, timeout)`
  * url 请求链接
  * data post请求时的参数
  * timeout 超时时间


````python
# -*- coding:utf8 -*-
import urllib2

# 向指定的url地址发起请求，并返回服务器响应的类文件对象
response = urllib2.urlopen("http://www.baidu.com")

# 服务器类文件对象支持Python文件对象的操作方法
# read()读取文件内容，返回字符串
html = response.read()

print html
````

**以上不支持自定义构造HTTP请求，默认User-Agent会是：Python-urllib**

* `Accept-Encoding` 不要写，避免数据压缩
* `User-Agent` 必须写，避免爬虫被识别
* `http://www.baidu.com/` 末尾斜杠加上

````python
# -*- coding:utf8 -*-
import urllib2

user_headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
}

#通过urllib2.Request()方法构造一个请求对象
request = urllib2.Request("http://www.baidu.com", headers=user_headers)

# 向指定的url地址发起请求，并返回服务器响应的类文件对象
response = urllib2.urlopen(request)

# 服务器类文件对象支持Python文件对象的操作方法
# read()读取文件内容，返回字符串
html = response.read()

print html
````

* `response.getcode()` 返回`http`请求的返回状态码
* `response.geturl()` 返回数据的是哪个URL，防止重定向
* `response.info()` 返回服务器响应的`http`报头

### 反爬虫

```python
# -*- coding:utf8 -*-
import urllib2
import random

url = "http://www.baidu.com"

ua_list = [
    "Mozilla/5.0(Macintosh;U;IntelMacOSX10_6_8;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50",
    "Mozilla/5.0(Windows;U;WindowsNT6.1;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50",
    "Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0",
    "Mozilla/5.0(Macintosh;IntelMacOSX10.6;rv:2.0.1)Gecko/20100101Firefox/4.0.1",
    "Opera/9.80(WindowsNT6.1;U;en)Presto/2.8.131Version/11.11",
    "Mozilla/5.0(Macintosh;IntelMacOSX10_7_0)AppleWebKit/535.11(KHTML,likeGecko)Chrome/17.0.963.56Safari/535.11",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Maxthon2.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;TencentTraveler4.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Trident/4.0;SE2.XMetaSr1.0;SE2.XMetaSr1.0;.NETCLR2.0.50727;SE2.XMetaSr1.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;360SE)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;AvantBrowser)"
]


# 在可能的User-Agent列表中随机选择一个User-Agent
user_agent = random.choice(ua_list)

# 构造一个请求
request = urllib2.Request(url)

# 添加/修改一个头信息
request.add_header('User-Agent', user_agent)

# 获取一个已有的报头值
print request.get_header("User-agent")

```

## URLENCODE














