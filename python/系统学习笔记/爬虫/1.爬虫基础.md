# 爬虫

**网页三大特征：**

* 网页都有自己唯一的URL（统一资源定位符）来进行定位
* 网页都使用HTML （超文本标记语言）来描述页面信息。
* 网页都使用HTTP/HTTPS（超文本传输协议）协议来传输HTML数据。

**爬虫的设计思路：**

* 首先确定需要爬取的网页URL地址。
* 通过HTTP/HTTP协议来获取对应的HTML页面。
* 提取HTML页面里有用的数据：
  * a. 如果是需要的数据，就保存起来。
  * b. 如果是页面里的其他URL，那就继续执行第二步。

**抓取库**

* `urllib`、`urllib2`、`requests`

**解析库**

* `re`、`xpath`、`BeautifulSoup4(bs4)`、`jsonpath` 、`pyquery`

**动态页面**

* `Selenium+Phantomjs` 模拟真实浏览器
* `Tesseract` 机器学习库，进行图像识别

**框架**

* `Scrapy（twisted）/Pyspider` 

**分布式策略**

* `scrapy-redis`

**爬虫分类**

1. 通用爬虫（搜索引擎）
2. 聚焦爬虫（程序员针对部分内容做的爬虫）

## urllib2 

网络抓取库，在3中你被改为`urllib.request`

默认库安装位置`/usr/lib/python2.7/urllib2.py`

`/usr/local/lib/python2.7/site-paceages/` pip安装后存的位置

* `urllib2.urlopen(url, data=, timeout)`
  * url 请求链接
  * data post请求时的参数
  * timeout 超时时间


````python
# -*- coding:utf8 -*-
import urllib2

# 向指定的url地址发起请求，并返回服务器响应的类文件对象
response = urllib2.urlopen("http://www.baidu.com")

# 服务器类文件对象支持Python文件对象的操作方法
# read()读取文件内容，返回字符串
html = response.read()

print html
````

**以上不支持自定义构造HTTP请求，默认User-Agent会是：Python-urllib**

* `Accept-Encoding` 不要写，避免数据压缩
* `User-Agent` 必须写，避免爬虫被识别
* `http://www.baidu.com/` 末尾斜杠加上

````python
# -*- coding:utf8 -*-
import urllib2

user_headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
}

#通过urllib2.Request()方法构造一个请求对象
request = urllib2.Request("http://www.baidu.com", headers=user_headers)

# 向指定的url地址发起请求，并返回服务器响应的类文件对象
response = urllib2.urlopen(request)

# 服务器类文件对象支持Python文件对象的操作方法
# read()读取文件内容，返回字符串
html = response.read()

print html
````

* `response.getcode()` 返回`http`请求的返回状态码
* `response.geturl()` 返回数据的是哪个URL，防止重定向
* `response.info()` 返回服务器响应的`http`报头

### 反爬虫

```python
# -*- coding:utf8 -*-
import urllib2
import random

url = "http://www.baidu.com"

ua_list = [
    "Mozilla/5.0(Macintosh;U;IntelMacOSX10_6_8;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50",
    "Mozilla/5.0(Windows;U;WindowsNT6.1;en-us)AppleWebKit/534.50(KHTML,likeGecko)Version/5.1Safari/534.50",
    "Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0",
    "Mozilla/5.0(Macintosh;IntelMacOSX10.6;rv:2.0.1)Gecko/20100101Firefox/4.0.1",
    "Opera/9.80(WindowsNT6.1;U;en)Presto/2.8.131Version/11.11",
    "Mozilla/5.0(Macintosh;IntelMacOSX10_7_0)AppleWebKit/535.11(KHTML,likeGecko)Chrome/17.0.963.56Safari/535.11",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Maxthon2.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;TencentTraveler4.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Trident/4.0;SE2.XMetaSr1.0;SE2.XMetaSr1.0;.NETCLR2.0.50727;SE2.XMetaSr1.0)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;360SE)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;AvantBrowser)"
]


# 在可能的User-Agent列表中随机选择一个User-Agent
user_agent = random.choice(ua_list)

# 构造一个请求
request = urllib2.Request(url)

# 添加/修改一个头信息
request.add_header('User-Agent', user_agent)

# 获取一个已有的报头值
print request.get_header("User-agent")

```

## urlencode

* `urllib.urlencode()` 接受的参数是个字典

  ````python
  import urllib
  world={"name": "辛丙亮"}
  urllib.urlencode(world)
  m = urllib.urlencode(world)
  print urllib.unquote(m)
  name=辛丙亮
  ````

* `urllib.unquote() ` 转换回

````python
# -*- coding:utf8 -*-
import urllib2
import urllib

url = "http://www.baidu.com/s?"

keyword = raw_input("请输入用户的查询：")

wd = {"wd": keyword}

wd = urllib.urlencode(wd)

fullUrl = url + wd

request = urllib2.Request(fullUrl)
response = urllib2.urlopen(request)
print response.read()
````

## GET和POST

````python
# -*- coding:utf8 -*-
import urllib2
import urllib

url = "http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule"
key_world = raw_input("请输入要翻译的文本:")

form_data={
"i": key_world,
"type": "AUTO",
"doctype":"json",
"xmlVersion": "1.8",
"keyfrom":"fanyi.web",
"action":"FY_BY_CLICKBUTTION",
"typoResult":"true"
}

headers = {
"Accept":"application/json, text/javascript, */*; q=0.01",
"X-Requested-With":"XMLHttpRequest",
"User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36",
"Content-Type":"application/x-www-form-urlencoded; charset=UTF-8"
}

form_data_encode = urllib.urlencode(form_data)
request = urllib2.Request(url=url, data=form_data_encode, headers=headers)
response = urllib2.urlopen(request)
print response.read()
````

## Ajax

同POST

## cookie

```python
# -*- coding:utf8 -*-
import urllib
import urllib2


url = 'http://www.renren.com/477182930/profile'

headers = {
"Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
"Host":"www.renren.com",
"Accept-Language":"zh-CN,zh;q=0.9",
"Connection":"keep-alive",
"Cookie":"anonymid=jbg3losyqqqiek; depovince=GW; jebecookies=8bcf7ac9-26d3-4017-ad5b-77b0fba085bd|||||; _r01_=1; ick_login=17783882-fd14-414b-8a08-1663950fe117; _de=BCE15886291E57C5B0771936531D403D; p=9dc3737732b56539578bcda6943cb3273; first_login_flag=1; ln_uact=15102724518; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=8cd63aa75136d5622eb31cdc5652baed3; societyguester=8cd63aa75136d5622eb31cdc5652baed3; id=812033673; xnsid=8844bbca; loginfrom=syshome; ch_id=10016; _ga=GA1.2.1048710234.1513837285; _gid=GA1.2.389901114.1513837285; wp_fold=0",
"User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
}


request = urllib2.Request(url=url, headers=headers)
response = urllib2.urlopen(request)

print response.read()
```

## HTTPS

```python
# 忽略证书
# -*- coding:utf8 -*-
import urllib2
import urllib
import ssl

# 2. 表示忽略未经核实的ssl证书认证
context = ssl._create_unverified_context();

url = "https://www.12306.cn/mormhweb/"

request = urllib2.Request(url)

# 在urlopen()中明确添加context
response = urllib2.urlopen(request, context=context)

print response.read()

```

## Handler处理器和自定义opener

urlopen()不支持代理、cookie等其他HTTP/HTTPS等高级功能

1. 使用相关的`gandler`处理器，来创建特定的功能处理器对象
2. 然后通过`urllib2.build_opener()` 方法使用这些处理器对象，创建opener对象
3. 使用自定义的opener对象，调用open()方法发送请求

```python
# -*- coding:utf8 -*-
import urllib2

# 构建一个HTTPHandler处理器对象
# http_handler = urllib2.HTTPHandler()
http_handler = urllib2.HTTPHandler(debuglevel=1) # 自动打开调试模式，打印收发报信息

# 调用build_opener()方法构建一个自定义的opener对象，参数是构建的处理器对象
opener = urllib2.build_opener(http_handler)

request = urllib2.Request("http://www.baidu.com/")

response = opener.open(request)

print response.read()
```

## 代理

````python
# -*- coding:utf8 -*-
import urllib2

proxySwitch = True #打开代理

# 构建一个handler处理器，参数是一个字典类型
#username:passwd@110.110.110.100：port
httpProxyHandler = urllib2.ProxyHandler({"http": "114.215.104.49:16816"})

# 没有代理的处理器对象
nullProxyHandler = urllib2.ProxyHandler({})

if proxySwitch:
    opener = urllib2.build_opener(httpProxyHandler)
else:
    opener = urllib2.build_opener(nullProxyHandler)

# 构建一个全局的opener,之后说有的请求都可以用urlopen打开
urllib2.install_opener(opener)

request = urllib2.Request("http://www.baidu.com/")
response = urllib2.urlopen(request)

print response.read()
````

## 代理和WEB客户端授权验证

服务器配置了登录名密码时

* `urllib2.HTTPPasswordMgrWithDefaultRealm()` 创建一个密码管理对象，用来保存Http请求相关授权信息
* `ProxyBasicAuthHandler()` 授权代理处理器
* `HTTPBasicAuthHandler()` 验证web客户端的授权处理器

```python
# -*- coding:utf8 -*-
import urllib2

test = "test"
passwd = '123456'
webserver = "192.168.21.56"

# 构建一个密码管理对象，用来保存和请求相关的授权账户信息
passwdMgr = urllib2.HTTPPasswordMgrWithDefaultRealm()

# 添加授权账户信息，第一个参数real如果没有指定就写None，站点IP，账户，密码
passwdMgr.add_password(None, webserver, test, passwd)

httpauth_handler = urllib2.HTTPBasicAuthHandler(passwdMgr)

opener = urllib2.build_opener(httpauth_handler) # 可以同时加上代理

request = urllib2.Request("http://"+webserver)

response = opener.open(request)

print response.read()
```

##模拟登录

### cookielib

该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。

> - CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。
> - FileCookieJar (filename,delayload=None,policy=None)：从CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。
> - MozillaCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与`Mozilla浏览器 cookies.txt兼容`的FileCookieJar实例。
> - LWPCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与`libwww-perl标准的 Set-Cookie3 文件格式`兼容的FileCookieJar实例。

```python
# -*- coding:utf8 -*-
import urllib2
import cookielib
import urllib

# 通过CookieJar()类来构建一个cookieJar()对象，用来执行cookie值
cookie = cookielib.CookieJar()

# 通过HTTPCookieProcess()处理器类构建一个处理器对象，来处理cookie
# 参数是构建的Cookiejar()对象
cookie_handler = urllib2.HTTPCookieProcessor(cookie)
# 构建一个自定义的opener
opener = urllib2.build_opener(cookie_handler)

# 自定义opener的参数，可以赋值Http报头参数
opener.addheaders = [
    ("User-Agent", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36")
]

url = 'http://www.renren.com/PLogin.do'

data = {
    "email": "",
    "password": ""
}

# 编码数据
data = urllib.urlencode(data)

request = urllib2.Request(url=url, data=data)

response = opener.open(request)

# print response.read()

#第二次请求会使用上一次请求保存的cookie
response_frind = opener.open('http://www.renren.com/360072054/profile')

print response_frind.read()
```

## re

python中的re模块有两种方式

* ````
  pattern = re.compile("\d") # 将正则表达式编译成一个Pattern规则对象

  pattern.match() 从开始位置查找，第一个符合规则的
  pattern.search() 从任意位置往后查找，返回第一个
  pattern.findall() 全部匹配项，返回列表
  pattern.finditer() 所有全部匹配，返回一个迭代器
  pattern.split() 分割字符串，返回列表
  pattern.sub() 替换
  ````


```python
# -*- coding:utf8 -*-
import re

pattern = re.compile(r'\d+')
print pattern.match(r'123456', 2, 5).group()
```

## 段子爬虫

```python
# -*- coding:utf8 -*-
import urllib2
import re


class Spider:
    def __init__(self):
        self.switch = True # 爬虫开关
        self.page = 1

    def load_page(self):
        """下载页面"""
        url = 'http://www.neihan8.com/article/list_5_'+str(self.page)+'.html'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
        }

        request = urllib2.Request(url=url, headers=headers)
        response = urllib2.urlopen(request)
        html = response.read()

        # 创建正则表达式规则对象，匹配每页里的段子内容,re.S表示匹配全部字符串
        pattern = re.compile('<div\sclass="f18\smb20">(.*?)</div>', re.S)

        content_list = pattern.findall(html)

        self.deal_page(content_list)

    def deal_page(self, content_list):
        """处理每个页面的段子"""
        for item in content_list:
            item = item.decode('gbk').encode('utf-8')
            item = item.replace("<p>", "").replace("</p>","").replace("<br />", "").replace("&ldquo;", "").replace("&rdquo;","").replace("\r\n", "").replace("&hellip;", "")
            self.write_page(item)

    def write_page(self, item):
        """内容写入文件"""
        with open('duanzi.txt', 'a+') as f:
            f.write(item)

    def start_work(self):
        """控制爬虫运行"""
        while self.switch:
            command = raw_input("按回车爬取下一页，退出按quit：")

            if 'quit' == command:
                self.switch = False
            else:
                self.load_page()
                self.page += 1



if __name__ == "__main__":
    obj = Spider()
    obj.start_work()
```

## lxml库

**爬虫开发时不要开启广告屏蔽工具**

* `//div[@class="threadlist_lz clearfix"]//a[@clss="j_th_tit"]/@`
* `//img[@class="BDE_Image"]?@src`

安装库`pip install lxml`

```python
# -*- coding:utf8 -*-
import urllib
import urllib2
from lxml import etree


url = 'https://tieba.baidu.com/f?kw=美女'

def down_page(url):
    request = urllib2.Request(url)
    html = urllib2.urlopen(request).read()
    # 解析HTML文档为HTML DOM模型
    content = etree.HTML(html)
    # print content
    # 返回所有匹配成功的列表集合
    link_list = content.xpath('//div[@class="t_con cleafix"]/div/div/div/a/@href')

    # link_list = content.xpath('//a[@class="j_th_tit"]/@href')
    for link in link_list:
        fulllink = "http://tieba.baidu.com" + link

        # print fulllink
        # 组合为每个帖子的链接
        # print link
        loadImage(fulllink)


def loadImage(link):
    user_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
    }

    request = urllib2.Request(url=link, headers=user_headers)

    response = urllib2.urlopen(request)

    html = response.read()

    # 解析HTML文档为DOM模型
    content = etree.HTML(html)
    # 所有匹配的集合
    image_link = content.xpath('//img[@class="BDE_Image"]/@src')

    writeImage(image_link)


def writeImage(urls):
    user_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
    }

    print urls

    for url in urls:
        request = urllib2.Request(url)

        # response = urllib2.urlopen(request)
        image = urllib2.urlopen(request).read()

        filename = './images/'+url[-12:]

        with open(filename, 'wb') as f:
            f.write(image)

        print filename
        f.close()

if "__main__" == __name__:
    down_page('https://tieba.baidu.com/f?kw=美女&pn=50')
```

## BeautifulSop

* `pip install bs4` 安装


````python
# -*- coding:utf8 -*-
import requests
from bs4 import BeautifulSoup
import time

def zhihuLogin(url = 'https://www.zhihu.com/#signin'):
    # 构建一个Session对象，可以保存Cookie
    sess = requests.Session()

    header = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
    }

    #获得登录页面，同时记录网页Cookie
    html = sess.get(url, headers=header).text

    #调用lxml库
    bs = BeautifulSoup(html, 'lxml')

    # _xsrf作用是防止CSRF攻击（跨站请求伪造，跨域攻击）的
    xsrf = bs.find("input", attrs={"name": "_xsrf"}).get("value")

    m_time = time.time()

    captcha_url = 'https://www.zhihu.com/captcha.gif?r=%d&type=login' % (m_time * 1000)

    captcha = get_captcha(captcha_url)
    data ={
        '_xsrf': xsrf,
        'email': '709464835@qq.com',
        'password': 'yjfc4883212',
        'captcha': captcha
    }

    login_url = 'https://www.zhihu.com/login/email'
    response = sess.post(login_url, data=data, headers=header)

    print response.text


def get_captcha(captcha_url):
    """
    验证码处理
    :param captcha_url:
    :return:
    """
    header = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
    }

    content = requests.get(captcha_url, headers = header).content
    filename = './images/' + str(int(time.time()*1000)) + '_captcha' + '.jpg'

    with open(filename, 'wb') as f:
        f.write(content)

    code = raw_input("请输入验证码:")

    return code



if '__main__' == __name__:
    zhihuLogin('https://www.zhihu.com/#signin')
````

## json

* `json.loads()` 把Json格式字符串解码转换成Python对象

  ```python
  # -*- coding:utf8 -*-
  import json

  strList = '[1, 2, 3, 4]'
  strDict =  '{"city": "北京", "name": "大猫"}'

  print json.loads(strList)
  print json.loads(strDict)
  ```

* `json.dumps()` python类型转化为json字符串，返回一个str对象 把一个Python对象编码转换成Json字符串

  ````python
  # -*- coding:utf8 -*-
  import json
  import chardet

  listStr = [1, 2, 3, 4]
  tupleStr = (1, 2, 3, 4)
  dictStr = {"city": "北京", "name": "大猫"}

  print json.dumps(listStr)
  print json.dumps(tupleStr)
  print json.dumps(dictStr)

  # 注意：json.dumps() 序列化时默认使用的ascii编码
  # 添加参数 ensure_ascii=False 禁用ascii编码，按utf-8编码
  # chardet.detect()返回字典, 其中confidence是检测精确度

  # 使用chardet识别字符的编码
  print chardet.detect(json.dumps(dictStr))
  ````

* `json.dump()` 将Python内置类型序列化为json对象后写入文件

  ```python
  # -*- coding:utf8 -*-
  import json

  listStr = [{"city": "北京"}, {"name": "大刘"}]
  dictStr = {"city": "北京", "name": "大刘"}

  json.dump(listStr, open("listStr.json","w"), ensure_ascii=False)
  json.dump(dictStr, open("dictStr.json","w"), ensure_ascii=False)
  ```

* `json.load()` 读取文件中json形式的字符串元素 转化成python类型

  ```python
  # -*- coding:utf8 -*-
  import json

  strList = json.load(open("listStr.json"))
  print strList
  ```

  ### jsonpath

  | XPath | JSONPath  |                    描述                    |
  | :---: | :-------: | :--------------------------------------: |
  |  `/`  |    `$`    |                   根节点                    |
  |  `.`  |    `@`    |                   现行节点                   |
  |  `/`  | `.`or`[]` |                   取子节点                   |
  | `..`  |    n/a    |             取父节点，Jsonpath未支持             |
  | `//`  |   `..`    |            就是不管位置，选择所有符合条件的条件            |
  |  `*`  |    `*`    |                 匹配所有元素节点                 |
  |  `@`  |    n/a    | 根据属性访问，Json不支持，因为Json是个Key-value递归结构，不需要。 |
  | `[]`  |   `[]`    |    迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等）    |
  |  \|   |   `[,]`   |                支持迭代器中做多选。                |
  | `[]`  |   `?()`   |                 支持过滤操作.                  |
  |  n/a  |   `()`    |                 支持表达式计算                  |
  | `()`  |    n/a    |              分组，JsonPath不支持              |

````python
# -*- coding:utf8 -*-
import urllib2
import json
import jsonpath

url = 'http://www.lagou.com/lbs/getAllCitySearchLabels.json'

header = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
}

request = urllib2.Request(url=url, headers=header)
response = urllib2.urlopen(request)

# 取出json文字字符串内容，返回格式是字符串
json_str = response.read()
# 转换为python形式unicode字符串
unicode_str = json.loads(json_str)

# python形式的列表
city_list = jsonpath.jsonpath(unicode_str, "$..name")

# 返回unicode字符串
array = json.dumps(city_list, ensure_ascii=False)



with open("lagoucity.json", "w") as f:
    f.write(array.encode('utf-8'))
````

## 糗事百科案例

```
IE 9.0
User-Agent:Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;
 
IE 8.0
User-Agent:Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)
 
IE 7.0
User-Agent:Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)
 
IE 6.0
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)
```

```python
# -*- coding:utf8 -*-

#取用户名
# '//div[contains(@id, "qiushi_tag_")]/div/a/h2/text()'

#取内容
# '//div[@class="content"]/span/text()'
# '//div[contains(@id, "qiushi_tag_")]/a/div[@class="content"]/span/text()'

#好笑数量
# '//div[contains(@id, "qiushi_tag_")]/div/span[@class="stats-vote"]/i/text()'

#取评论数
# '//div[contains(@id, "qiushi_tag_")]/div/span[@class="stats-comments"]/a/i/text()'

#取内容图片
# '//div[contains(@id, "qiushi_tag_")]/div[@class="thumb"]/a/img/@src'

import urllib2
from lxml import etree
import json

url = 'https://www.qiushibaike.com/8hr/page/2/'
header = {
    "User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0"
}

request = urllib2.Request(url, headers = header)

respoonse = urllib2.urlopen(request)

html = respoonse.read()

# 响应返回的是字符串，解析为HTML DOM模式
text = etree.HTML(html)

# 返回所有节点的根节点，使用模糊查询
node_list = text.xpath('//div[contains(@id, "qiushi_tag_")]')

items = {}

for node in node_list:
    # 用户名
    username = node.xpath('./div/a/h2/text()')
    username = username[0].strip()



    # 图片链接
    img_str = node.xpath('./div[@class="thumb"]/a/img/@src')

    if img_str != []:
        img_src = img_str[0]
    else:
        img_src = ''

    # 段子内容
    content = node.xpath('./a/div[@class="content"]/span/text()')[0]
    content = content.strip()

    # 点赞
    vote = node.xpath('./div/span[@class="stats-comments"]/a/i/text()')[0]

    # 评论
    comments = node.xpath('./div/span[@class="stats-comments"]/a/i/text()')[0]

    items = {
        "username": username,
        "images": img_src,
        "content": content,
        "vote": vote,
        "comments": comments
    }

    with open("qiushi.json", "a+") as f:
        str = json.dumps(items, ensure_ascii=False).encode("utf-8") + '\n'
        f.write(str)
```

## 多线程

* GIL：python中的执行通行证，而且只有一个，拿到就可以执行程序
* python多线程 适合大量I/O处理
* python多进程适合进行密集计算

```python
# -*- coding:utf-8 -*-
import threading
from Queue import Queue
from lxml import etree
import requests
import json


class ThreadCrawl(threading.Thread):
    """
    采集线程类
    """
    def __init__(self, threadName, pageQueue, dataQueue):
        # threading.Thread.__init__(self)
        super(ThreadCrawl, self).__init__()
        self.threadName = threadName
        self.pageQueue = pageQueue
        self.dataQueue = dataQueue
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"
        }

    def run(self):
        while not CRAWL_EXIT:
            try:
                # 取出一个数字，先进先出
                # 可选参数block，默认为True
                # 1.如果队列为空，block=true,进入阻塞状态，直到队列有新输出，不会结束
                # 2.如果队列为空，block=false,弹出一个Queue.empty()的异常
                page = self.pageQueue.get(True)
                url = 'https://www.qiushibaike.com/8hr/page/' + str(page) + '/'

                content = requests.get(url, headers=self.headers)
                self.dataQueue.put(content)
            except:
                pass

CRAWL_EXIT = False
PARSE_EXIT = False


def main():
    # 页码的队列，表示10个页面
    pageQueue = Queue(10)


    # 放入1~10的数字
    for i in range(1, 11):
        pageQueue.put(i)

    # 采集结果(每页的HTML源码)的数据队列，参数为空表示不限制
    dataQueue = Queue()

    # 采集线程名字
    crawList = ['采集线程1', '采集线程2号', '采集线程3号']


    threadcrawl = []
    for threadName in crawList:
        thread = ThreadCrawl(threadName, pageQueue, dataQueue)
        thread.start()
        threadcrawl.append(thread) # 存储采集线程

    #等待pageQueue队列为空，也就是等待之前的操作执行完毕
    while not pageQueue.empty():
        pass

    # 如果pageQueue为空，采集线程退出循环
    global CRAWL_EXIT
    CRAWL_EXIT = True

    print 'pageQueue为空'


if __name__ == '__main__':
    main()

```
