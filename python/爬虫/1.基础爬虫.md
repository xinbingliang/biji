# 简单爬虫









## urllib介绍

* `urllib.urlopen` 打开一个远程的http连接，并进行读取
  * `url` 请求的地址
  * data 不为空，变成POST方法，格式必须是`application/x-www-form-urlencoded`
  * 返回类文件句柄
    * `read(size)` 不指定size则全部读取
    * `readline()` 读取一行
    * `readlines()` 读取所有行 并形成列表
    * `close()` 关闭
    * `getcode()` 返回HTTP应答码

```python
# python2.7
import urllib


def demo():
    s = urllib.urlopen('http://blog.kamidox.com') #协议是必须的
    # print(s.readline())
    # print(s.read())
    # print(s.readlines())
    # print(s.getcode())
    
    # for i in range(10):
    #     print('line %d: %s' %(i, s.readline()))
    print(s.read(100))


if __name__ == '__main__':
    demo()
```

* HTTPMessage方法
  * info() 返回httplib.HTTPMessage实例
  * httplib.HTTPMessage
    * headers http头
    * gettype() 媒体类型
    * getheader()/getheaders() 获得头字段
    * items() /keys()/values()  所有的头

```python
import urllib

def print_list(list):
    for i in list:
        print(i)


def demo():
    s = urllib.urlopen('http://blog.kamidox.com')
    msg = s.info()
    # print(msg)
    # print_list(msg.headers) #所有HTTP头内容
    # print_list(msg.items()) #头信息组成元组
    print(msg.getheader('Content-type')) #获取摸一个信息
    print_list(dir(msg))

if __name__ == '__main__':
    demo()
```

* urllib.urlretrieve(将远程文件下载到本地)
  * `url` 远程地址
  * `filename` 要保存到本地的文件 
  * `reporthook` 下载状态报告
    * 参数1：当前传输的块数
    * 参数2：块大小
    * 参数3：数据总大小
    * 需要注意：`content-length` 不是必须的
  * `data` POST的`application/x-www-form-urlencoded` 格式的数据
  * 返回(filename，HTTPMessage)

```python
def progress(blk, blk_size, total_size):    #total_size包含头信息
    print('%d/%d - %.02f' % (blk * blk_size, total_size, (float)(blk * blk_size)*100 / total_size))

def retrieve():
    filename, msg = urllib.urlretrieve('http://blog.kamidox.com', 'data.html', reporthook=progress)
    # print filename
    # print msg

if __name__ == '__main__':
    retrieve()
```

* `urllib.urlencode`

  * 把字典数据转化为URL编码
  * 用途
    * 对`ur`l参数进行编码
    * 对`post`上去的form数据进行编码

  ```python
  import urllib

  def urlencode():
      params = {'score': 100, 'name': '爬虫基础', 'comment': 'very good'}
      qs = urllib.urlencode(params)
      print qs

  if  __name__ == '__main__':
      urlencode()
  #comment=very+good&score=100&name=%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80
  ```

  * `urlparse.parse_qs`
    * 把`URL`编码转化为字典数据

  ```python
  import urllib
  import urlparse
  ```


  def urldencode():
      params = {'score': 100, 'name': '爬虫基础', 'comment': 'very good'}
      qs = urllib.urlencode(params)
      d = urlparse.parse_qs(qs)
      print d

  if  __name__ == '__main__':
      urlencode()
  ```

  * `urlparse.urlparse(url)` 对`URL`解析为字典
  * 其他
    * quote
    * unquote
    * pathname2url
    * url2pathname

  #### 股票数据获取

  * http://table.finance.yahoo.com/table.csv?s=000001.sz  深圳股指 
  * http://table.finance.yahoo.com/table.csv?s=600000.ss  上证股指
    * a ,b,c 开始时间
    * d,e,f 结束时间
    * s股票代码
  * http://table.finance.yahoo.com/table.csv?a=0&b=1&c=2012&d=3&e=19&f=2012&s=600690.ss
    * 月-日-年
    * 时间-开盘价-最高价-最低价-收盘价-成交量-Adj close

  ```python
  import urllib
  def download_stock_data(stock_list):
      for sid in stock_list:
          url = 'http://table.finance.yahoo.com/table.csv?s=' + sid
          fname = sid + '.csv'
          urllib.urlretrieve(url, fname)


  if  __name__ == '__main__':
      stock_list = ['300001.sz', '300002.sz']
      download_stock_data(stock_list)
  ```

````python
  import urllib
  import datetime

  def download_stock_data(stock_list):
      for sid in stock_list:
          url = 'http://table.finance.yahoo.com/table.csv?s=' + sid
          fname = sid + '.csv'
          urllib.urlretrieve(url, fname)


  def download_stock_data_in_period(stock_list, start, end):
      for sid in stock_list:
          params = {'a': start.month - 1, 'b': start.day, 'c': start.year,
                    'd': end.month - 1, 'e': end.day, 'f': end.year, 's': sid}
          url = 'http://table.finance.yahoo.com/table.csv?' + urllib.urlencode(params)
          fname = '%s_%d%d%d_%d%d%d.csv' % (sid, start.month - 1, start.day, start.year, end.month - 1, end.day, end.year)
          urllib.urlretrieve(url, fname)


  if __name__ == '__main__':
      stock_list = ['300001.sz', '300002.sz']
      end = datetime.date(year=2017, month=3, day = 7)
      start = datetime.date(year=2016, month=12, day=1)
      download_stock_data_in_period(stock_list, start, end)
````

  ## urllib2

* `urllib2.Request` 提供htp header定能力
* 提供强大的功能，包括cookie处理，鉴权，可定制化
* `urllib.urlencode` 在 `urllib.urlencode` 没有实现

### 函数和功能

* `urllib.urlopen`
  * url
  * data
  * timeout 超时时间

````python
import urllib2

def urlopen():
    url = 'http://blog.kamidox.com/sfhsdj'
    try:
        s = urllib2.urlopen(url, timeout=3)
        print(s.read(100))
    except urllib2.HTTPError, e:
        print(e)
    else:
        print(s.read(100))
        s.close()

if __name__ == '__main__':
    urlopen()
````

* `urllib2.Request()`
  * `url`
  * `data-optional`
  * `headers` - 字典
  * 使用Request添加或修改http头部
    * `Accept: application/json`
    * `Content-Type:application/json`
    * `User-Agent:Chorme`

```python
import urllib2

def request():
    # 定制 HTTP 头
    headers = {'User-Agent': 'Mozilla/5.0', 'x-my-header': 'my value'}
    req = urllib2.Request('http://blog.kamidox.com', headers=headers)
    s = urllib2.urlopen(req);
    print(s.read(100))
    print(req.headers)
    s.close()


if __name__ == '__main__':
    request()
```

* `urllib2.build.opener`
  * `BaseHandler` 及其子类
    * `HTTPHandler`
    * `HTTPSHandler`
    * `HTTPCookieProcessor`
  * build_opener
    * 参数Handler列表
    * 返回`OpenerDirector`
  * 默认会创建的Handler链
    * `proxyHandler`(如果设置了代理)
    * `Unknownhandler`
    * `HTTPHandler`
    * `HTTPDefaultErrorHandler` 错误请求
    * `HTTPRedirectHandler` 跳转的请求
    * `FTPHandler` 
    * `FileHandler` 本地文件打开
    * `HTTPErrorProcessor` 
    * `HTTPSHandler`(如果安装 了ssl模块)
  * 示例 request_post_debug
    * 打印`http`信息
    * POST 数据
  * 保存opener为默认 
    * urllib2.install_opener

````python
import urllib2
import urllib

def request():
    # 定制 HTTP 头
    headers = {'User-Agent': 'Mozilla/5.0'}
    data = {'username': '709464835@qq.com', 'password': 'yjfc4883212'}
    req = urllib2.Request('http://www.douban.com/', data=urllib.urlencode(data), headers=headers)

    opener = urllib2.build_opener(urllib2.HTTPHandler(debuglevel=1))    #打印与服务器交互的信息
    s = opener.open(req)
    print(s.read(1000))
    s.close()


if __name__ == '__main__':
    request()
````

**安装调试handler**

````python
def request():
    headers = {'User-Agent': 'Mozilla/5.0'}
    req = urllib2.Request('http://blog.kamidox.com', headers=headers)
    s = urllib2.urlopen(req)
    print(s.read(100))
    print(req.headers)
    s.close()


def install_debug_handler():
    opener = urllib2.build_opener(urllib2.HTTPHandler(debuglevel=1), urllib2.HTTPSHandler(debuglevel=1))
    urllib2.install_opener(opener)

if __name__ == '__main__':
    install_debug_handler()
    request()

````

### cookie处理

* `cookielib.CookieJar`
  * 提供解析并保存`cookie`的接口
* HTTPCookieProcessor
  * 提供自动处理cookie功能

```python
import urllib2
import cookielib

def handle_cookie():
    cookiejar = cookielib.CookieJar()
    handler = urllib2.HTTPCookieProcessor(cookiejar=cookiejar)
    opener = urllib2.build_opener(handler, urllib2.HTTPHandler(debuglevel=1))
    s = opener.open('http://www.douban.com')
    print(s.read(1000))
    s.close()

    print('=*'*80)
    print(cookiejar._cookies)
    print('=*'*80)

    s = opener.open('http://www.douban.com')    #第二次请求会携带该cookie
    s.close()

if __name__ == '__main__':
    handle_cookie()
```

## 豆瓣爬取电影

* HTMLParser
  * feed 向解析器喂数据，可以分段提供
  * handle_starttag 处理html的开始标签
    * tag 标签名称
    * attrs 属性列表
  * handle_data 处理标签里的数据体
    * data 数据文本

**爬取豆瓣热播**

````python
# -*- coding: utf-8 -*-
import urllib2
from HTMLParser import HTMLParser

# 定义的解析器
class MovieParser(HTMLParser):
    def __init__(self):
        HTMLParser.__init__(self)
        self.movies = []

    def handle_starttag(self, tag, attrs):
        def _attr(attrlist, attrname):
            for attr in attrlist:
                if attr[0] == attrname:  #若解析出来的名称和要获得的属性名称相同就返回属性
                    return attr[1]
            return  None

        if tag == 'li' and _attr(attrs, 'data-title') and _attr(attrs, 'data-category') == 'nowplaying':
            movie = {}
            movie['title'] = _attr(attrs, 'data-title') #电影名称
            movie['score'] = _attr(attrs, 'data-score') #电影评分
            movie['director'] = _attr(attrs, 'data-director') #电影导演
            movie['actors'] = _attr(attrs, 'data-actors') #电影主演

            self.movies.append(movie)
            print('%(title)s|%(score)s|%(director)s|(actors)s' % movie)

# 当前地区热播电影
def nowplaying_movies(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.59 Safari/537.36'}
    req = urllib2.Request(url, headers=headers)
    s = urllib2.urlopen(req)
    parser = MovieParser()
    parser.feed(s.read())
    s.close()
    return parser.movies

if __name__ == '__main__':
    url = 'http://movie.douban.com/nowplaying/wuhan/'
    movies = nowplaying_movies(url)

    # print(movies)

    import json
    print('%s' % json.dumps(movies, sort_keys=True, indent=4, separators=(',', ': ')))

````

## Request库

> 第三方实现的HTTP库

* requests.request
  * `method`: get/post/head/put/delete
  * `url`
  * `params` 请求参数
  * `data` 字典，字节流或类文件句柄
  * `json` 上传的json数据
  * headers 自定义htttp头
  * `cookies` 发送额外的cookies
  * `verify` 是否检验证书
* requests.get
  * url
  * 和request的参数一样
* requests.post
  * url
  * data
  * json
  * 和request的参数
* requests.head
* requests.put
* requests.delete


* requests.Response
  * `status_code` 状态码
  * `headers` 应答的`http`头
  * `json` 应答的`json`数据
  * text应答的`unicode`编码文本
  * `content`应答的字节流数据
  * `cookie`应答的`cookies`自动处理

```python
import requests

def get_json():
    r = requests.get('https://api.github.com/events')
    # print(r.status_code)
    print(r.headers)
    print(r.content)
    print(r.text)
    print(r.json())

if __name__ == '__main__':
    get_json()
```

**定制请求**

````python
# -*- coding: utf-8 -*-
import requests

# 定制请求
def get_querystring():
    url = 'http://httpbin.org/get'  # http协议测试
    params = {'qs1': 'value1', 'qs2': 'value2'}
    r = requests.get(url, params=params)
    print(r.status_code)
    print(r.content)


if __name__ == '__main__':
    get_json()
````

**定制请求头**

````python
# -*- coding: utf-8 -*-
import requests

# 定制头
def get_querystring():
    url = 'http://httpbin.org/get'  # http协议测试
    headers = {'x-header1': 'value1', 'x-header2': 'value2'}
    r = requests.get(url, headers=headers)
    print(r.status_code)
    print(r.headers)
    print(r.content)
    print(r.text)

if __name__ == '__main__':
    get_querystring()
````

**处理cookie**

```python
# -*- coding: utf-8 -*-
import requests

def get_cookie():
    headers = {'User-Agent': 'Chrome'}
    url = 'http://www.douban.com'
    r = requests.get(url, headers=headers)
    print(r.status_code)
    print(r.cookies)

if __name__ == '__main__':
    get_cookie()
```

**高级用法**

* Session：同一个会话内参数保持一致，且会重用TCP连接来提高性能。也会尽量保持连接提高性能
* SSL证书认证：开启，关闭，自定义CA证书
* 上传普通文件和复杂结构文件
* 代理访问

[Requests中文文档](http://docs.python-requests.org/zh_CN/latest/user/quickstart.html#id9)

**登录豆瓣**

````python
# -*- coding: utf-8 -*-
import requests
from HTMLParser import HTMLParser

class DoubanClient(object):
    def __init__(self):
        object.__init__(self)
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.59 Safari/537.36',
                   'Origin': 'https://accounts.douban.com'}
        self.session = requests.Session()
        self.session.headers.update(headers)


    def login(self, username, password,
              source='index_nav',
              redir="http://www.douban.com/", login="登录"):
        url = 'https://www.douban.com/accounts/login'

        r = self.session.get(url)
        (captcha_id, captcha_url) = _get_captcha(r.content)

        if captcha_id:
            captcha_solution = raw_input('please input solution for [%s]:' % captcha_url)



        data = {
            'form_email': username,
            'form_password': password,
            'source': source,
            'redir': redir
        }

        if captcha_id:
            data['captcha-id'] = captcha_id
            data['captcha-solution'] = captcha_solution

        headers = {'referer': 'http://www.douban.com/accounts/login?source=main',
                   'host': 'accounts.douban.com'}

        r = self.session.post(url, data=data, headers=headers)
        print(self.session.cookies.items())

        pass

    def edit_signature(self, username, signature):
        pass

def _attr(attrs, attrname):
    for attr in attrs:
        if attr[0] == attrname:
            return attr[1]
    return None

def _get_captcha(content):
    class CaptchaParser(HTMLParser):
        def __init__(self):
            HTMLParser.__init__(self)
            self.captcha_id = None
            self.captcha_url = None


        def handle_starttag(self, tag, attrs):
            if tag == 'input' and _attr(attrs, 'type') == 'hidden' and _attr(attrs, 'name') == 'captche-id':
                self.captcha_id = _attr(attrs, 'value')

            if tag == 'img' and _attr(attrs, 'id') == 'captcha_image' and _attr(attrs, 'class') == 'captcha_image':
                self.captcha_url = _attr(attrs, 'src')


    p = CaptchaParser()
    p.feed(content)
    return p.captcha_id, p.captcha_url


if __name__ == '__main__':
    c = DoubanClient()
    c.login('709464835@qq.com',  '*****')
````

**修改签名**

```python
# -*- coding: utf-8 -*-
import requests
from HTMLParser import HTMLParser

class DoubanClient(object):
    def __init__(self):
        object.__init__(self)
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.59 Safari/537.36',
                   'Origin': 'https://accounts.douban.com'}
        self.session = requests.Session()
        self.session.headers.update(headers)


    def login(self, username, password,
              source='index_nav',
              redir="http://www.douban.com/", login="登录"):
        url = 'https://www.douban.com/accounts/login'

        r = self.session.get(url)
        (captcha_id, captcha_url) = _get_captcha(r.content)

        if captcha_id:
            captcha_solution = raw_input('please input solution for [%s]:' % captcha_url)



        data = {
            'form_email': username,
            'form_password': password,
            'source': source,
            'redir': redir
        }

        if captcha_id:
            data['captcha-id'] = captcha_id
            data['captcha-solution'] = captcha_solution

        headers = {'referer': 'http://www.douban.com/accounts/login?source=main',
                   'host': 'accounts.douban.com'}

        r = self.session.post(url, data=data, headers=headers)
        print(self.session.cookies.items())

        pass

    def edit_signature(self, username, signature):
        url = 'https://www.douban.com/people/%s/' % username
        r = self.session.get(url) #获得用户主页
        ck = _get_ck(r.content)

        url = 'https://www.douban.com/j/people/%s/edit_signature' % username
        headers = {
            'referer': url,
            'host': 'www.douban.com',
            'x-requested-with': 'XMLHttpRequest'
        }

        data = {'ck': ck, 'signature': signature}
        r = self.session.post(url, data=data, headers=headers)
        print(r.content)



def _get_ck(content):
    class CKParser(HTMLParser):
        def __init__(self):
            HTMLParser.__init__(self)
            self.ck = None

        def handle_starttag(self, tag, attrs):
            if tag == 'input' and _attr(attrs, 'type') == 'hidden' and _attr(attrs, 'name') == 'ck':
                self.ck = _attr(attrs, 'value')

    p = CKParser()
    p.feed(content)
    return  p.ck

def _attr(attrs, attrname):
    for attr in attrs:
        if attr[0] == attrname:
            return attr[1]
    return None

def _get_captcha(content):
    class CaptchaParser(HTMLParser):
        def __init__(self):
            HTMLParser.__init__(self)
            self.captcha_id = None
            self.captcha_url = None


        def handle_starttag(self, tag, attrs):
            if tag == 'input' and _attr(attrs, 'type') == 'hidden' and _attr(attrs, 'name') == 'captche-id':
                self.captcha_id = _attr(attrs, 'value')

            if tag == 'img' and _attr(attrs, 'id') == 'captcha_image' and _attr(attrs, 'class') == 'captcha_image':
                self.captcha_url = _attr(attrs, 'src')


    p = CaptchaParser()
    p.feed(content)
    return p.captcha_id, p.captcha_url


if __name__ == '__main__':
    c = DoubanClient()
    c.login('709464835@qq.com',  '***********')
    c.edit_signature('124814629', '贴近生活，感受彼此!')
```

## 使用requests重构豆瓣热播并下载海报

```python
# -*- coding:utf-8 -*-
import requests
from HTMLParser import HTMLParser

class MovieParser(HTMLParser):
    def __init__(self):
        HTMLParser.__init__(self)
        self.movies = []
        self.in_movies = False

    def handle_starttag(self, tag, attrs):
        def _attr(attrList, attrname):
            for attr in attrList:
                if attr[0] == attrname:
                    return  attr[1]
            return None
        if tag == 'li' and _attr(attrs, 'data-title') and _attr(attrs, 'data-category') == 'nowplaying':
            movie = {}
            movie['title'] = _attr(attrs, 'data-title')
            movie['score'] = _attr(attrs, 'data-score')
            movie['director'] = _attr(attrs, 'data-director')
            movie['actors'] = _attr(attrs, 'data-actors')
            self.movies.append(movie)
            print('%(title)s|%(score)s|%(director)s|%(actors)s|' % movie)
            self.in_movies = True

        if tag == 'img' and self.in_movies:
            self.in_movies = False
            src = _attr(attrs, 'src')
            movie = self.movies[len(self.movies)-1]
            movie['poster-url'] = src
            _download_poster_image(movie)

def _download_poster_image(movie):
    src = movie['poster-url']
    r = requests.get(src)
    fname = 'img/' + src.split('/')[-1]
    with open(fname, 'wb') as f:
        f.write(r.content)
        movie['poster-path'] = fname



def nowplaying_movies(url):
    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.59 Safari/537.36'}
    r = requests.get(url, headers=headers)
    parser = MovieParser()
    parser.feed(r.content)
    return parser.movies


if __name__ == '__main__':
    url = 'https://movie.douban.com/nowplaying/wuhan/'
    movies = nowplaying_movies(url)

    import json
    print('%s' % json.dumps(movies, sort_keys=True, indent=4, separators=(',', '：')))
```



















